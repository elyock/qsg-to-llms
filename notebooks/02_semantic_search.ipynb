{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b088054-daa2-4dbe-a71c-3a3dd58ab3b5",
   "metadata": {
    "id": "9b088054-daa2-4dbe-a71c-3a3dd58ab3b5"
   },
   "source": [
    "# Part 1: Setting up a basic semantic search system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722572bc",
   "metadata": {},
   "source": [
    "A big thank you to [David on Github](https://github.com/gypsydave5) for finding a bug in my analysis code and bringing it to my attention. The bug was resolved! \n",
    "\n",
    "Some numbers might be different from what is reported in the book/video course but the overall gist is the same: re-ranking helps our semantic search and fine-tuning the re-ranking cross encoder yielded even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aGixSoAgIzLo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone openai sentence-transformers tiktoken datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12182ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datetime import datetime, timezone\n",
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "import logging\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0289c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key = os.environ.get('PINECONE_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "INDEX_NAME = 'semantic-search-test'\n",
    "NAMESPACE = 'default'\n",
    "ENGINE = 'text-embedding-3-large'  # has vector size 3072\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=pinecone_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf993a3-6d51-49f4-8968-60f654d6202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get lists of embeddings from the OpenAI API\n",
    "def get_embeddings(texts, engine=ENGINE):\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=engine\n",
    "    )\n",
    "\n",
    "    return [d.embedding for d in list(response.data)]\n",
    "\n",
    "def get_embedding(text, engine=ENGINE):\n",
    "    return get_embeddings([text], engine)[0]\n",
    "\n",
    "len(get_embedding('hi')), len(get_embeddings(['hi', 'hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f'Creating index {INDEX_NAME}')\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,  # The name of the index\n",
    "        dimension=3072,  # The dimensionality of the vectors for our OpenAI embedder\n",
    "        metric='cosine',  # The similarity metric to use when searching the index\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Store the index as a variable\n",
    "index = pc.Index(name=INDEX_NAME)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd86f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pinecone(texts, engine=ENGINE):\n",
    "    # Get the current UTC date and time\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    # Generate vector embeddings for each string in the input list, using the specified engine\n",
    "    embeddings = get_embeddings(texts, engine=engine)\n",
    "\n",
    "    # Create tuples of (hash, embedding, metadata) for each input string and its corresponding vector embedding\n",
    "    # The my_hash() function is used to generate a unique hash for each string, and the datetime.utcnow() function is used to generate the current UTC date and time\n",
    "    return [\n",
    "        (\n",
    "            my_hash(text),  # A unique ID for each string, generated using the my_hash() function\n",
    "            embedding,  # The vector embedding of the string\n",
    "            dict(text=text, date_uploaded=now)  # A dictionary of metadata, including the original text and the current UTC date and time\n",
    "        )\n",
    "        for text, embedding in zip(texts, embeddings)  # Iterate over each input string and its corresponding vector embedding\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id, embedding, metadata = prepare_for_pinecone(texts)[0]\n",
    "\n",
    "print('ID:  ',_id, '\\nLEN: ', len(embedding), '\\nMETA:', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49debd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_pinecone(texts, namespace=NAMESPACE, batch_size=None, show_progress_bar=False):\n",
    "    # Call the prepare_for_pinecone function to prepare the input texts for indexing\n",
    "    total_upserted = 0\n",
    "    if not batch_size:\n",
    "        batch_size = len(texts)\n",
    "\n",
    "    _range = range(0, len(texts), batch_size)\n",
    "    for i in tqdm(_range) if show_progress_bar else _range:\n",
    "        batch = texts[i: i + batch_size]\n",
    "        prepared_texts = prepare_for_pinecone(batch)\n",
    "\n",
    "        # Use the upsert() method of the index object to upload the prepared texts to Pinecone\n",
    "        total_upserted += index.upsert(\n",
    "            vectors=prepared_texts,\n",
    "            namespace=namespace\n",
    "        )['upserted_count']\n",
    "\n",
    "\n",
    "    return total_upserted\n",
    "\n",
    "# Call the upload_texts_to_pinecone() function with the input texts\n",
    "upload_texts_to_pinecone(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V0XI6RAom-Ln",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_pinecone(query, top_k=3, include_metadata=True):\n",
    "    # get embedding from THE SAME embedder as the documents\n",
    "    query_embedding = get_embedding(query, engine=ENGINE)\n",
    "\n",
    "    return index.query(\n",
    "      vector=query_embedding,\n",
    "      top_k=top_k,\n",
    "      namespace=NAMESPACE,\n",
    "      include_metadata=include_metadata   # gets the metadata (dates, text, etc)\n",
    "    ).get('matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_texts_from_pinecone(texts, namespace=NAMESPACE):\n",
    "    # Compute the hash (id) for each text\n",
    "    hashes = [hashlib.md5(text.encode()).hexdigest() for text in texts]\n",
    "\n",
    "    # The ids parameter is used to specify the list of IDs (hashes) to delete\n",
    "    return index.delete(ids=hashes, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5172f-a0c6-4692-ab77-83321e141679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523cab6-0bc3-4791-9c73-7bd7d7e5858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xtreme\", \"MLQA.en.en\")\n",
    "\n",
    "# rename test -> train and val -> test (as we will use it in later in this chapter)\n",
    "dataset['train'] = dataset['test']\n",
    "dataset['test'] = dataset['validation']\n",
    "del dataset['validation']\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622fbfa-2dde-4711-9c46-1390eb3430f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0], dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passages = list(set(dataset['test']['context']))\n",
    "for idx in tqdm(range(0, len(unique_passages), 32)):\n",
    "    passages = unique_passages[idx:idx + 32]\n",
    "    upload_texts_to_pinecone(passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a7c80-7149-430e-b22c-9926c0d1daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33yf7QrWtwt-",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b021b-2cdb-4d73-a2c6-f2e0d1eb5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e13316-2295-4a72-8f2a-b459f0e7826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_from_pinecone('Does an infection for Sandflies go away over time?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92699e-2d92-4083-9508-ae47355c9c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ab80e-f4b4-48d2-8aef-b522024f6658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e180a25-37e4-47a8-8ae8-a49f39c106ab",
   "metadata": {
    "id": "7e180a25-37e4-47a8-8ae8-a49f39c106ab"
   },
   "source": [
    "# Part 2: Making results more relevant with a cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you didn't import before\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def get_results_from_pinecone(query, top_k=3, re_rank_model=None, verbose=True, correct_hash=None):\n",
    "\n",
    "    results_from_pinecone = query_from_pinecone(query, top_k=top_k)\n",
    "\n",
    "    if not results_from_pinecone:\n",
    "        return []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Query:\", query)\n",
    "\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    retrieved_correct_position, reranked_correct_position = None, None\n",
    "    for idx, result_from_pinecone in enumerate(results_from_pinecone):\n",
    "        if correct_hash and result_from_pinecone['id'] == correct_hash:\n",
    "            retrieved_correct_position = idx\n",
    "\n",
    "    if re_rank_model is not None:\n",
    "        if verbose:\n",
    "            print('Document ID (Hash)\\t\\tRetrieval Score\\tCE Score\\tText')\n",
    "\n",
    "        sentence_combinations = [[query, result_from_pinecone['metadata']['text']] for result_from_pinecone in results_from_pinecone]\n",
    "\n",
    "        # Compute the similarity scores for these combinations\n",
    "        similarity_scores = re_rank_model.predict(sentence_combinations, activation_fct=nn.Sigmoid())\n",
    "\n",
    "        # Sort the scores in decreasing order\n",
    "        sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "        sim_scores_sort = list(reversed(np.sort(similarity_scores)))\n",
    "        top_re_rank_score = sim_scores_sort[0]\n",
    "\n",
    "        # Print the scores\n",
    "        # print(list(zip(sim_scores_argsort, sim_scores_sort)))\n",
    "        for idx, _ in enumerate(sim_scores_argsort):\n",
    "            result_from_pinecone = results_from_pinecone[_]\n",
    "            if correct_hash and retrieved_correct_position == _:\n",
    "                reranked_correct_position = idx\n",
    "            final_results.append({'score': similarity_scores[idx], 'id': result_from_pinecone['id'], 'metadata': result_from_pinecone['metadata']})\n",
    "            if verbose:\n",
    "                print(f\"{result_from_pinecone['id']}\\t{result_from_pinecone['score']:.2f}\\t{similarity_scores[idx]:.6f}\\t{result_from_pinecone['metadata']['text'][:50]}\")\n",
    "        return {'final_results': final_results, 'retrieved_correct_position': retrieved_correct_position, 'reranked_correct_position': reranked_correct_position, 'results_from_pinecone': results_from_pinecone, 'top_re_rank_score': top_re_rank_score}\n",
    "\n",
    "    if verbose:\n",
    "        print('Document ID (Hash)\\t\\tRetrieval Score\\tText')\n",
    "    for result_from_pinecone in results_from_pinecone:\n",
    "        final_results.append(result_from_pinecone)\n",
    "        if verbose:\n",
    "            print(f\"{result_from_pinecone['id']}\\t{result_from_pinecone['score']:.2f}\\t{result_from_pinecone['metadata']['text'][:50]}\")\n",
    "\n",
    "    return {'final_results': final_results, 'retrieved_correct_position': retrieved_correct_position, 'reranked_correct_position': reranked_correct_position}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_GPhx4MMIVqU",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a8a8f-e873-43e7-9f56-b57bdf328416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained cross encoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', num_labels=1)\n",
    "\n",
    "q_to_hash = {data['question']: my_hash(data['context']) for data in dataset['test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vptFN0wPw0uS",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_inputs = list(set(dataset['test']['question']))\n",
    "len(unique_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56237c-f9d6-4b3d-a8ac-31098cf4f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = unique_inputs[0]\n",
    "print(query)\n",
    "\n",
    "for t in dataset['test']:\n",
    "    if t['question'] == query:\n",
    "        print(t['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0da5ac-0345-45e2-9913-188590a31522",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = get_results_from_pinecone(\n",
    "    query,\n",
    "    top_k=2, # grab 2 results\n",
    "    re_rank_model=cross_encoder,\n",
    "    correct_hash=q_to_hash[query],\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "query_result['retrieved_correct_position'], query_result['reranked_correct_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb3b3c-9baa-4298-abd8-0e7b75dd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result  # the right context isn't there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa66ac8-d69c-49a1-9f55-e9ef93379fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = get_results_from_pinecone(\n",
    "    query,\n",
    "    top_k=100, # grab 10 results\n",
    "    re_rank_model=cross_encoder, correct_hash=q_to_hash[query],\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "query_result['retrieved_correct_position'], query_result['reranked_correct_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uk1lhptdUfrL",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ec53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98413a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for question in tqdm(test_sample['question']):\n",
    "    r = get_results_from_pinecone(\n",
    "        question, top_k=TOP_K, re_rank_model=cross_encoder, correct_hash=q_to_hash[question],\n",
    "        verbose=False\n",
    "        )\n",
    "\n",
    "    r['retrieved_correct_position'], r['reranked_correct_position']\n",
    "    predictions.append(r)\n",
    "    if len(predictions) % 100 == 0:\n",
    "        retrieved_accuracy = sum([_['retrieved_correct_position'] == 0 for _ in predictions])/len(predictions)\n",
    "        re_ranked_accuracy = sum([_['reranked_correct_position'] == 0 for _ in predictions])/len(predictions)\n",
    "\n",
    "        print(f'Accuracy without re-ranking: {retrieved_accuracy}')\n",
    "        print(f'Accuracy with re-ranking: {re_ranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff94e7-0b78-40ca-8cf0-42e345439e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_accuracy = sum([_['retrieved_correct_position'] == 0 for _ in predictions])/len(predictions)\n",
    "re_ranked_accuracy = sum([_['reranked_correct_position'] == 0 for _ in predictions])/len(predictions)\n",
    "\n",
    "print(f'Accuracy without re-ranking: {retrieved_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {re_ranked_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fT4FuVcf9ONw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649afac-9434-440b-b332-de9149a7ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831397d3-606a-4c7f-b77f-6ee85bd24f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[['retrieved_correct_position', 'reranked_correct_position']].mean()  # lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eyv-6pPIEy0r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do recall @ 1, 3, 5, 10, etc\n",
    "X = [1, 3, 5, 10, 25, 50]\n",
    "OPENAI_RETRIEVAL = []\n",
    "OLD_CROSS_ENCODER = []\n",
    "\n",
    "for k in X:\n",
    "    embedding_only_recall = predictions_df[predictions_df['retrieved_correct_position'] < k].shape[0]\n",
    "    reranked_recall = predictions_df[predictions_df['reranked_correct_position'] < k].shape[0]\n",
    "    OPENAI_RETRIEVAL.append(embedding_only_recall / predictions_df.shape[0])\n",
    "    OLD_CROSS_ENCODER.append(reranked_recall / predictions_df.shape[0])\n",
    "    print(k, embedding_only_recall, reranked_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9aef05-e271-41e0-a8ce-88b6c81bed4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f14809e",
   "metadata": {
    "id": "3f14809e"
   },
   "source": [
    "## OPEN SOURCE ALTERNATIVE TO EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed58d3a-bed5-4f70-a4d6-3d624ab98868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# load up our open source embedding model\n",
    "bi_encoder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# bi_encoder = SentenceTransformer(\"sentence-transformers/msmarco-MiniLM-L-6-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode query and documents\n",
    "docs = dataset['test']['context']\n",
    "doc_emb = bi_encoder.encode(docs, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec2a2d-9ca6-4f5e-a0db-3be16c2363ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df00998-34d8-4f0b-9527-510d9515b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "# Function to find most similar document\n",
    "def find_most_similar(embedder, text, embeddings, documents, k=3):\n",
    "    query_embedding = embedder.encode([text], show_progress_bar=False)\n",
    "    similarities = semantic_search(query_embedding, embeddings, top_k=k)\n",
    "    return [(documents[sim['corpus_id']], sim['score'], sim['corpus_id']) for sim in similarities[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0afee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "query = sample(dataset['test']['question'], 1)[0]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93280be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ranking_open_source(embedder, doc_emb, query, top_k=3, re_rank_model=None):\n",
    "    ans = {'retrieved_correct_position': None}\n",
    "    correct_hash = q_to_hash[query]\n",
    "    results = find_most_similar(embedder, query, doc_emb, docs, k=top_k)\n",
    "    for idx, (passage, score, doc_idx) in enumerate(results):\n",
    "        if correct_hash == my_hash(passage):\n",
    "            ans['retrieved_correct_position'] =  idx\n",
    "    if re_rank_model is not None:\n",
    "        ans['reranked_correct_position'] = None\n",
    "        sentence_combinations = [(query, r[0]) for r in results]\n",
    "\n",
    "        # Compute the similarity scores for these combinations\n",
    "        similarity_scores = re_rank_model.predict(sentence_combinations, activation_fct=nn.Sigmoid())\n",
    "\n",
    "        # Sort the scores in decreasing order\n",
    "        sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "        for i, idx in enumerate(sim_scores_argsort):\n",
    "            r = results[idx]\n",
    "            if correct_hash and my_hash(r[0]) == correct_hash:\n",
    "                ans['reranked_correct_position'] = i\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f198cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ranking_open_source(bi_encoder, doc_emb, query, top_k=TOP_K, re_rank_model=cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389511fa-38d3-4538-987d-1ff8a83ac85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "os_predictions = []\n",
    "\n",
    "for i, question in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "    os_predictions.append(eval_ranking_open_source(bi_encoder, doc_emb, question['question'], top_k=TOP_K, re_rank_model=cross_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f031c-4965-4aed-aecd-0a2593f55115",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_predictions_df = pd.DataFrame(os_predictions)\n",
    "os_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88419a17-598b-4b3e-b314-e1c5ef3475eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accuracy = sum([p['retrieved_correct_position'] == 0 for p in os_predictions])/len(os_predictions)\n",
    "reranked_accuracy = sum([p['reranked_correct_position'] == 0 for p in os_predictions])/len(os_predictions)\n",
    "\n",
    "print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fc44d-6af1-48b6-b859-01a1698c7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do recall @ 1, 3, 5, 10\n",
    "OPEN_SOURCE_RETRIEVAL = []\n",
    "OPEN_SOURCE_RETRIEVAL_PLUS_PRE_CE = []\n",
    "for k in X:\n",
    "    embedding_only_recall = os_predictions_df[os_predictions_df['retrieved_correct_position'] < k].shape[0]\n",
    "    reranked_recall = os_predictions_df[os_predictions_df['reranked_correct_position'] < k].shape[0]\n",
    "    print(k, embedding_only_recall, reranked_recall)\n",
    "    OPEN_SOURCE_RETRIEVAL.append(embedding_only_recall / os_predictions_df.shape[0])\n",
    "    OPEN_SOURCE_RETRIEVAL_PLUS_PRE_CE.append(reranked_recall / os_predictions_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733061c-eb98-40cb-b116-602e1c0dcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, OPENAI_RETRIEVAL, label='OAI Retrieval Only', marker='o')\n",
    "plt.plot(X, OLD_CROSS_ENCODER, label='OAI + Pretrained CE', marker='s')\n",
    "\n",
    "plt.plot(X, OPEN_SOURCE_RETRIEVAL, label='OS Retrieval Only', marker='*')\n",
    "plt.plot(X, OPEN_SOURCE_RETRIEVAL_PLUS_PRE_CE, label='OS + Pretrained CE', marker='^')\n",
    "\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Comparing embedding models + pre-trained vs fine-tuned CE (all retrieved 50 results then re-ranked)')\n",
    "plt.xlabel('Recall @')\n",
    "plt.ylabel('Performance')\n",
    "plt.xticks(X)\n",
    "plt.yticks([i/100 for i in range(70, 101, 5)])  # Adjusting y-ticks to start from 0.75\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fe3bc-4fc1-4e69-bf60-6ab4f75da968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e5dcd-7ad8-43d0-aa1b-2b94b3784253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3a30f-e913-43ad-9fa7-f712abfa59d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6c207-7f75-4b04-8d32-541265902fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec04ddb-5a7d-4506-a0ba-a3ddedf985df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5cb27-018c-4299-a91f-6b39293c583e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3200a66-9fc0-4f39-94e8-3456b85b6a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea30153c",
   "metadata": {
    "id": "ea30153c"
   },
   "source": [
    "## Advanced: Fine-tuning the re-ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58979a1e-0fe1-488f-9805-56efd499278d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508edaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd5c19-894a-41e4-bf6f-d9e1197c053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_passages = list(set(dataset['train']['context']))\n",
    "len(unique_train_passages), len(dataset['train']['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaad504-b944-4e36-987f-34126189c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_train_passages), doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d5ef0-3fe3-4ed8-87c0-1cea3d9abcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sentence_transformers.util.semantic_search\n",
    "train_doc_embed = bi_encoder.encode(unique_train_passages, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d79fe9-f053-40e7-a2e6-2efd715b43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_passages = np.array(unique_train_passages)\n",
    "\n",
    "# Example usage\n",
    "print(unique_train_passages[0])\n",
    "\n",
    "find_most_similar(bi_encoder, unique_train_passages[0], train_doc_embed, unique_train_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e4ca4-0bfe-4243-9534-49370ed66b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative example mining\n",
    "train_samples = []\n",
    "\n",
    "for train_example in tqdm(dataset['train']):\n",
    "    # train_samples.append(\n",
    "    #         InputExample(\n",
    "    #             texts=[train_example['question'], train_example['context']], label=1\n",
    "    #         )\n",
    "    #     )\n",
    "    for i, (passage, score, corpus_idx) in enumerate(find_most_similar(bi_encoder, train_example['question'], train_doc_embed, unique_train_passages)):\n",
    "\n",
    "        train_samples.append(\n",
    "            InputExample(\n",
    "                texts=[train_example['question'], passage], label=int(passage == train_example['context'])\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "shuffle(train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281241b-54ed-4405-a101-f774efd2317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series([t.label for t in train_samples]).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LOZkS9bSgwga",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7eaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples[2].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator, CEBinaryClassificationEvaluator\n",
    "import math\n",
    "import torch\n",
    "from random import sample\n",
    "\n",
    "logger.setLevel(logging.DEBUG)  # just to get some logs\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model_save_path = './fine_tuned_ir_cross_encoder'\n",
    "\n",
    "train_dataloader = DataLoader(train_samples[:int(len(train_samples)*.8)], shuffle=True, batch_size=16)\n",
    "\n",
    "# An evaluator for training performance\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(train_samples[int(len(train_samples)*.8):], name='test')\n",
    "\n",
    "# Rule of thumb for warmup steps\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "print(f\"Warmup-steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e8079-1335-4d53-944d-3ed0f6537781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_samples:\n",
    "    if t.label == 1:\n",
    "        print('Example of label 1')\n",
    "        print(t.__dict__, cross_encoder.predict(t.texts, activation_fct=nn.Sigmoid()))\n",
    "        break\n",
    "for t in train_samples:\n",
    "    if t.label == 0:\n",
    "        print('Example of label 0')\n",
    "        print(t.__dict__, cross_encoder.predict(t.texts, activation_fct=nn.Sigmoid()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981bd03-bb31-492d-aabf-d5e6c7647704",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "\n",
    "# you may turn on debug for more logs here e.g. logger.setLevel(logging.DEBUG)\n",
    "cross_encoder.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    loss_fct=nn.BCEWithLogitsLoss(),  # this is the default loss if num_labels is 1 otherwise CrossEntropyLoss\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    use_amp=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9_a5R9sVBvHN",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f747537",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = CrossEncoder(model_save_path)\n",
    "\n",
    "print(finetuned.predict(['hello', 'hi'], activation_fct=nn.Sigmoid()))\n",
    "print(finetuned.predict(['hello', 'hi'], activation_fct=nn.Identity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "ft_predictions = []\n",
    "\n",
    "for question in tqdm(test_sample['question']):\n",
    "    r = get_results_from_pinecone(\n",
    "        question, top_k=TOP_K, re_rank_model=finetuned, correct_hash=q_to_hash[question],\n",
    "        verbose=False\n",
    "        )\n",
    "\n",
    "    r['retrieved_correct_position'], r['reranked_correct_position']\n",
    "    ft_predictions.append(r)\n",
    "    if len(ft_predictions) % 100 == 0:\n",
    "        retrieved_accuracy = sum([_['retrieved_correct_position'] == 0 for _ in ft_predictions])/len(ft_predictions)\n",
    "        re_ranked_accuracy = sum([_['reranked_correct_position'] == 0 for _ in ft_predictions])/len(ft_predictions)\n",
    "\n",
    "        print(f'Accuracy without re-ranking: {retrieved_accuracy}')\n",
    "        print(f'Accuracy with re-ranking: {re_ranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b782d-ef97-4c0a-be53-aefea5936676",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_accuracy = sum([_['retrieved_correct_position'] == 0 for _ in ft_predictions])/len(ft_predictions)\n",
    "re_ranked_accuracy = sum([_['reranked_correct_position'] == 0 for _ in ft_predictions])/len(ft_predictions)\n",
    "\n",
    "print(f'Accuracy without re-ranking: {retrieved_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {re_ranked_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking got slightly better after 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcd92e-31ad-4bf7-9e30-b6d260a06871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_predictions_df = pd.DataFrame(ft_predictions)\n",
    "ft_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Sr2q5IN9LMz",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_predictions_df[['retrieved_correct_position', 'reranked_correct_position']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_predictions_df[['retrieved_correct_position', 'reranked_correct_position']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do recall @ 1, 3, 5, 10\n",
    "OPENAI_RETRIEVAL = []\n",
    "OPENAI_RETRIEVAL_PLUS_FT_CE = []\n",
    "for k in X:\n",
    "    embedding_only_recall = ft_predictions_df[ft_predictions_df['retrieved_correct_position'] < k].shape[0]\n",
    "    reranked_recall = ft_predictions_df[ft_predictions_df['reranked_correct_position'] < k].shape[0]\n",
    "    OPENAI_RETRIEVAL.append(embedding_only_recall / ft_predictions_df.shape[0])\n",
    "    OPENAI_RETRIEVAL_PLUS_FT_CE.append(reranked_recall / ft_predictions_df.shape[0])\n",
    "    print(k, embedding_only_recall, reranked_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5e922-7667-4a02-b4fc-7acb9cc97c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "os_predictions = []\n",
    "\n",
    "for i, question in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "    os_predictions.append(eval_ranking_open_source(bi_encoder, doc_emb, question['question'], top_k=TOP_K, re_rank_model=finetuned))\n",
    "\n",
    "os_predictions_df = pd.DataFrame(os_predictions)\n",
    "os_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaeb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_accuracy = sum([p['retrieved_correct_position'] == 0 for p in os_predictions])/len(os_predictions)\n",
    "reranked_accuracy = sum([p['reranked_correct_position'] == 0 for p in os_predictions])/len(os_predictions)\n",
    "\n",
    "print(f'Accuracy without re-ranking: {raw_accuracy}')\n",
    "print(f'Accuracy with re-ranking: {reranked_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T00iM9SyaUFI",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do recall @ 1, 3, 5, 10\n",
    "OPEN_SOURCE_RETRIEVAL = []\n",
    "OPEN_SOURCE_RETRIEVAL_PLUS_FT_CE = []\n",
    "for k in X:\n",
    "    embedding_only_recall = os_predictions_df[os_predictions_df['retrieved_correct_position'] < k].shape[0]\n",
    "    reranked_recall = os_predictions_df[os_predictions_df['reranked_correct_position'] < k].shape[0]\n",
    "    print(k, embedding_only_recall, reranked_recall)\n",
    "    OPEN_SOURCE_RETRIEVAL.append(embedding_only_recall / os_predictions_df.shape[0])\n",
    "    OPEN_SOURCE_RETRIEVAL_PLUS_FT_CE.append(reranked_recall / os_predictions_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ub8HU8BZAW1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, OPENAI_RETRIEVAL, label='OAI Retrieval Only', marker='o')\n",
    "plt.plot(X, OPEN_SOURCE_RETRIEVAL, label='OS Retrieval Only', marker='*')\n",
    "plt.plot(X, OPEN_SOURCE_RETRIEVAL_PLUS_PRE_CE, label='OS + Pretrained CE', marker='^')\n",
    "\n",
    "plt.plot(X, OPEN_SOURCE_RETRIEVAL_PLUS_FT_CE, label='OS + Finetuned CE', marker='v')\n",
    "plt.plot(X, OLD_CROSS_ENCODER, label='OAI + Pretrained CE', marker='s')\n",
    "plt.plot(X, OPENAI_RETRIEVAL_PLUS_FT_CE, label='OAI + Finetuned CE', marker='d')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Comparing embedding models + pre-trained vs fine-tuned CE (all retrieved 50 results then re-ranked)')\n",
    "plt.xlabel('Recall @')\n",
    "plt.ylabel('Performance')\n",
    "plt.xticks(X)\n",
    "plt.yticks([i/100 for i in range(70, 101, 5)])  # Adjusting y-ticks to start from 0.75\n",
    "\n",
    "# Adding legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('recall_at_k.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9oGQvj0xZAwo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results as a table\n",
    "\n",
    "results_df = pd.DataFrame({'RECALL @': [1, 3, 5, 10, 25, 50], 'OS_Retrieval_Only': OPEN_SOURCE_RETRIEVAL, 'OS_Retrieval_Plus_Finetuned_CE': OPEN_SOURCE_RETRIEVAL_PLUS_FT_CE   , 'OAI_Retrieval_Only': OPENAI_RETRIEVAL    , 'OAI_Retrieval_Plus_Pretrained_CE': OLD_CROSS_ENCODER, 'OAI_Retrieval_Plus_Finetuned_CE': OPENAI_RETRIEVAL_PLUS_FT_CE})\n",
    "results_df.sort_values(by='RECALL @')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OFno8WyIZDR6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "methods = [\n",
    "    \"OS_Retrieval_Only\",\n",
    "    \"OS_Retrieval_Plus_Finetuned_CE\",\n",
    "    \"OAI_Retrieval_Only\",\n",
    "    \"OAI_Retrieval_Plus_Pretrained_CE\",\n",
    "    \"OAI_Retrieval_Plus_Finetuned_CE\"\n",
    "]\n",
    "\n",
    "recalls = [0.501742, 0.642857, 0.753484, 0.833624, 0.890244]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(methods, recalls, color=\"skyblue\")\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + 0.005,\n",
    "        f\"{height:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Labeling and aesthetics\n",
    "plt.title(\"Recall@1 Across Different Methods\")\n",
    "plt.ylabel(\"Recall@1\")\n",
    "plt.xticks(rotation=25, ha=\"right\")  # Rotate x-axis labels if needed\n",
    "plt.ylim([0, 1])                    # Since recall values typically range [0,1]\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sIA-PtwdIEv3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
